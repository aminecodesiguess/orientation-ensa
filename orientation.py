import streamlit as st
import tempfile
import os
from langchain_groq import ChatGroq
from langchain_community.document_loaders import PyPDFLoader
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS

# --- TA CL√â API ---
# Connexion au coffre-fort Streamlit
GROQ_API_KEY = st.secrets["GROQ_API_KEY"]

# --- CONFIGURATION DE LA PAGE ---
st.set_page_config(page_title="Orientation ENSA Tanger", page_icon="üéì")

# --- HEADER (Gestion d'erreur de l'image) ---
col1, col2 = st.columns([1, 4])
with col1:
    if os.path.exists("logo.png"):
        st.image("logo.png", width=100)
    else:
        st.markdown("# üè´")
with col2:
    st.title("Assistant Orientation ENSAT")
    st.markdown("**National School of Applied Sciences of Tangier**")

st.divider()

# --- INITIALISATION DE LA M√âMOIRE (NOUVEAU) ---
# Si l'historique n'existe pas encore, on cr√©e une liste vide
if "messages" not in st.session_state:
    st.session_state.messages = []

# --- BARRE LAT√âRALE ---
with st.sidebar:
    st.header("üìö Documents")
    uploaded_files = st.file_uploader(
        "Chargez les PDF ici",
        type="pdf",
        accept_multiple_files=True
    )
    process_btn = st.button("Analyser les documents")

# --- LOGIQUE D'ANALYSE (Inchang√©e) ---
if process_btn:
    if not GROQ_API_KEY.startswith("gsk_"):
        st.error("‚ö†Ô∏è ATTENTION : Tu n'as pas remplac√© la cl√© API dans le code !")
        st.stop()

    if not uploaded_files:
        st.warning("‚ö†Ô∏è Veuillez d'abord uploader des fichiers PDF.")
        st.stop()

    with st.spinner("Analyse en cours..."):
        try:
            all_docs = []
            for uploaded_file in uploaded_files:
                with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
                    tmp_file.write(uploaded_file.read())
                    tmp_path = tmp_file.name

                loader = PyPDFLoader(tmp_path)
                docs = loader.load()
                all_docs.extend(docs)
                os.remove(tmp_path)

            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
            splits = text_splitter.split_documents(all_docs)
            embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

            # On stocke la base de donn√©es vectorielle
            st.session_state.vectorstore = FAISS.from_documents(splits, embeddings)
            st.success("‚úÖ Analyse termin√©e ! Pose ta premi√®re question.")

        except Exception as e:
            st.error(f"Erreur technique : {e}")

# --- ZONE DE CHAT (AM√âLIOR√âE) ---

# 1. On v√©rifie si la base vectorielle est pr√™te
if "vectorstore" in st.session_state:

    # 2. On affiche TOUS les messages pr√©c√©dents de l'historique
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # 3. On capture la NOUVELLE question
    if prompt := st.chat_input("Ex: Quelles sont les fili√®res disponibles ?"):
        # A. On affiche la question de l'utilisateur tout de suite
        with st.chat_message("user"):
            st.markdown(prompt)
        # B. On l'ajoute √† la m√©moire
        st.session_state.messages.append({"role": "user", "content": prompt})

        # C. G√©n√©ration de la r√©ponse
        with st.chat_message("assistant"):
            with st.spinner("R√©flexion..."):
                # R√©cup√©ration du contexte
                vectorstore = st.session_state.vectorstore
                retriever = vectorstore.as_retriever()
                relevant_docs = retriever.invoke(prompt)
                context = "\n\n".join([doc.page_content for doc in relevant_docs])

                # Appel √† l'IA
                llm = ChatGroq(groq_api_key=GROQ_API_KEY, model_name="llama-3.3-70b-versatile")

                # Prompt syst√®me
                system_prompt = f"""Tu es un expert de l'ENSA Tanger.
                Utilise le contexte suivant pour r√©pondre √† la question.
                Si tu ne sais pas, dis-le.

                Contexte : {context}
                Question : {prompt}
                """

                response = llm.invoke(system_prompt)
                response_text = response.content

                # Affichage de la r√©ponse
                st.markdown(response_text)

        # D. On ajoute la r√©ponse de l'IA √† la m√©moire
        st.session_state.messages.append({"role": "assistant", "content": response_text})

elif not uploaded_files:
    # Message d'accueil si rien n'est charg√©

    st.info("üëà Commencez par charger vos documents PDF dans le menu √† gauche.")
