import streamlit as st
import os
from langchain_groq import ChatGroq
from langchain_community.document_loaders import PyPDFLoader
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS

# --- 1. CONFIGURATION & S√âCURIT√â ---
try:
    GROQ_API_KEY = st.secrets["GROQ_API_KEY"]
except:
    # Fallback si pas de secrets (pour test local rapide)
    st.error("Cl√© API non trouv√©e dans les secrets.")
    st.stop()

st.set_page_config(page_title="Orientation ENSA Tanger", page_icon="üéì")

# --- 2. HEADER ---
col1, col2 = st.columns([1, 4])
with col1:
    if os.path.exists("logo.png"):
        st.image("logo.png", width=100)
    else:
        st.markdown("# üè´")
with col2:
    st.title("Assistant Orientation ENSAT")
    st.markdown("**National School of Applied Sciences of Tangier**")

st.divider()

# --- 3. GESTION DE L'√âTAT (SESSION STATE) ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# Nouvelle variable pour savoir si on affiche le QCM ou le Chat
if "show_quiz" not in st.session_state:
    st.session_state.show_quiz = False

# --- 4. CHARGEMENT DES DONN√âES (OPTIMIS√â) ---
@st.cache_resource(show_spinner=False)
def initialize_vectorstore():
    folder_path = "data"
    all_docs = []
    
    if not os.path.exists(folder_path):
        return None, "Le dossier 'data' n'existe pas."
    
    files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]
    
    if not files:
        return None, "Aucun fichier PDF trouv√©."

    try:
        for filename in files:
            file_path = os.path.join(folder_path, filename)
            loader = PyPDFLoader(file_path)
            docs = loader.load()
            all_docs.extend(docs)
            
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
        splits = text_splitter.split_documents(all_docs)
        embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
        vectorstore = FAISS.from_documents(splits, embeddings)
        return vectorstore, None
        
    except Exception as e:
        return None, str(e)

with st.spinner("Chargement de la base de connaissances..."):
    vectorstore, error_msg = initialize_vectorstore()

if error_msg:
    st.error(f"Erreur : {error_msg}")
    st.stop()

st.session_state.vectorstore = vectorstore

# --- 5. BARRE LAT√âRALE ---
with st.sidebar:
    st.header("üéØ Menu")
    
    # Bouton pour lancer le QCM
    if st.button("üìù Passer le Test d'Orientation"):
        st.session_state.show_quiz = True
    
    # Bouton pour revenir au Chat normal
    if st.button("üí¨ Retour au Chat"):
        st.session_state.show_quiz = False
        
    st.divider()
    if st.button("üóëÔ∏è Effacer la conversation"):
        st.session_state.messages = []
        st.rerun()

# --- 6. LOGIQUE PRINCIPALE : QCM ou CHAT ? ---

if st.session_state.show_quiz:
    # --- A. MODE QCM (NOUVEAU) ---
    st.markdown("### üìù Test de Personnalit√© & Orientation (10 Questions)")
    st.info("R√©pondez spontan√©ment. L'IA analysera vos r√©ponses pour trouver votre fili√®re id√©ale.")

    with st.form("quiz_form"):
        # Les 10 Questions Strat√©giques
        q1 = st.radio("1. Qu'est-ce qui vous passionne le plus ?", 
                      ["Comprendre comment fonctionnent les choses (Th√©orie)", "Fabriquer et construire des choses (Pratique)", "G√©rer des projets et des √©quipes", "Le monde du num√©rique et du code"])
        
        q2 = st.select_slider("2. Aimez-vous les Math√©matiques ?", options=["Pas du tout", "Moyen", "J'aime bien", "J'adore"])
        
        q3 = st.radio("3. Quel type d'environnement de travail pr√©f√©rez-vous ?", 
                      ["Bureau calme devant un ordinateur", "Terrain / Chantier / Usine", "Laboratoire de recherche", "R√©unions et Management"])
        
        q4 = st.radio("4. Face √† un probl√®me, vous √™tes plut√¥t :", 
                      ["Analytique (Je cherche la cause logique)", "Cr√©atif (J'invente une solution nouvelle)", "Pragmatique (Je veux que √ßa marche vite)", "Organis√© (Je planifie la r√©solution)"])
        
        q5 = st.radio("5. Quel domaine vous attire le moins ?", 
                      ["La Chimie et la Biologie", "L'Informatique", "La M√©canique et l'√âlectricit√©", "L'√âconomie et la Gestion"])
        
        q6 = st.radio("6. Aimez-vous programmer / coder ?", ["Non, √ßa m'ennuie", "Un peu, par curiosit√©", "Oui, je pourrais y passer des heures"])
        
        q7 = st.radio("7. L'√©cologie et l'environnement sont pour vous :", ["Un sujet int√©ressant", "Une priorit√© absolue dans mon futur m√©tier", "Secondaire par rapport √† la technologie"])
        
        q8 = st.radio("8. Pr√©f√©rez-vous travailler sur :", ["Du logiciel (Virtuel)", "Du mat√©riel (Hardware, Machines, Robots)", "Des processus (Organisation, Logistique)"])
        
        q9 = st.radio("9. Comment g√©rez-vous le stress ?", ["Je panique un peu", "Je reste calme et concentr√©", "J'ai besoin d'action"])
        
        q10 = st.text_input("10. En un mot, quel est votre m√©tier de r√™ve ? (ex: Chef de projet, Data Scientist, Ing√©nieur BTP...)")

        submitted = st.form_submit_button("üéì Analyser mes r√©ponses")

        if submitted:
            with st.spinner("L'IA croise vos r√©ponses avec les fili√®res de l'ENSA..."):
                # 1. R√©cup√©ration contexte
                retriever = vectorstore.as_retriever()
                relevant_docs = retriever.invoke("Liste des fili√®res g√©nie informatique industriel civil t√©l√©com √©co")
                context_filieres = "\n\n".join([doc.page_content for doc in relevant_docs])

                # 2. Construction du Prompt avec les r√©ponses du QCM
                quiz_summary = f"""
                R1 (Passion): {q1}
                R2 (Maths): {q2}
                R3 (Environnement): {q3}
                R4 (R√©solution): {q4}
                R5 (Aime moins): {q5}
                R6 (Code): {q6}
                R7 (√âcologie): {q7}
                R8 (Support): {q8}
                R9 (Stress): {q9}
                R10 (R√™ve): {q10}
                """

                final_prompt = f"""
                Tu es un conseiller d'orientation expert de l'ENSA Tanger.
                
                MISSION : 
                Analyse les r√©ponses de l'√©tudiant au QCM ci-dessous.
                D√©duis son profil psychologique et technique.
                Recommande-lui LA fili√®re la plus adapt√©e parmi celles disponibles dans le contexte.

                REPONSES DE L'√âTUDIANT (QCM) :
                {quiz_summary}

                CONTEXTE DES FILI√àRES DISPONIBLES :
                {context_filieres}

                FORMAT DE LA R√âPONSE :
                1. üß† **Analyse de Profil** : Tes points forts et int√©r√™ts d√©tect√©s.
                2. üèÜ **Fili√®re Recommand√©e** : Le nom pr√©cis de la fili√®re.
                3. üöÄ **Pourquoi ce choix ?** : Explication d√©taill√©e faisant le lien entre le QCM et la fili√®re.
                """

                # 3. Appel IA
                llm = ChatGroq(groq_api_key=GROQ_API_KEY, model_name="llama-3.3-70b-versatile")
                response = llm.invoke(final_prompt)
                
                # 4. Affichage
                st.success("Analyse termin√©e !")
                st.markdown(response.content)
                
                # Ajout √† l'historique pour qu'on puisse en discuter apr√®s
                st.session_state.messages.append({"role": "assistant", "content": f"**R√©sultat du Test QCM :**\n{response.content}"})
                st.balloons() # Petit effet visuel sympa

else:
    # --- B. MODE CHAT (ANCIEN CODE) ---
    # Affichage historique
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Input Chat
    if prompt := st.chat_input("Posez une question sur l'√©cole ou sur votre r√©sultat..."):
        with st.chat_message("user"):
            st.markdown(prompt)
        st.session_state.messages.append({"role": "user", "content": prompt})

        with st.chat_message("assistant"):
            with st.spinner("..."):
                retriever = vectorstore.as_retriever()
                relevant_docs = retriever.invoke(prompt)
                context = "\n\n".join([doc.page_content for doc in relevant_docs])

                llm = ChatGroq(groq_api_key=GROQ_API_KEY, model_name="llama-3.3-70b-versatile")
                
                # Prompt syst√®me qui prend en compte l'historique r√©cent pour la coh√©rence
                system_prompt = f"""Tu es un expert de l'ENSA Tanger.
                Utilise le contexte suivant pour r√©pondre.
                Contexte : {context}
                Question : {prompt}
                """
                
                response = llm.invoke(system_prompt)
                st.markdown(response.content)
        
        st.session_state.messages.append({"role": "assistant", "content": response.content})
